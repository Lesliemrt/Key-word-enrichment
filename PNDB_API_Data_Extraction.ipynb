{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PNDB API Data Extraction Pipeline\n",
    "\n",
    "This notebook extracts biodiversity data from the French National Biodiversity Database (PNDB) using their OpenDataSoft API and saves it to CSV format.\n",
    "\n",
    "## Overview\n",
    "- **Data Source**: PNDB (Pôle National de Données de Biodiversité)\n",
    "- **API**: OpenDataSoft platform v1.0\n",
    "- **Endpoint**: https://pndb.opendatasoft.com/api/datasets/1.0/search/\n",
    "- **Goal**: Extract Title, description, metadata about biodiversity datasets for further analysis\n",
    "- **Output**: Structured CSV file with dataset information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "from typing import List, Dict, Any\n",
    "import re\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. API Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Base URL: https://pndb.opendatasoft.com/api/datasets/1.0/search/\n",
      "Using API Key: 7de48c5bd7100022a452...\n",
      "Initial parameters: {'rows': 100, 'start': 0, 'apikey': '7de48c5bd7100022a4527661269206a9e92125f940f32dcd749b5da1'}\n"
     ]
    }
   ],
   "source": [
    "# PNDB API Configuration\n",
    "BASE_URL = \"https://pndb.opendatasoft.com/api/datasets/1.0/search/\"\n",
    "API_KEY = \"7de48c5bd7100022a4527661269206a9e92125f940f32dcd749b5da1\"\n",
    "\n",
    "# API Parameters (OpenDataSoft v1.0 format)\n",
    "PARAMS = {\n",
    "    'rows': 100,\n",
    "    'start': 0,\n",
    "    'apikey': API_KEY\n",
    "}\n",
    "\n",
    "# Headers for the API request\n",
    "HEADERS = {\n",
    "    'User-Agent': 'PNDB-Data-Extractor/1.0',\n",
    "    'Accept': 'application/json'\n",
    "}\n",
    "\n",
    "print(f\"API Base URL: {BASE_URL}\")\n",
    "print(f\"Using API Key: {API_KEY[:20]}...\")\n",
    "print(f\"Initial parameters: {PARAMS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test API Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing API connection...\n",
      "API Connection Successful\n",
      "Status Code: 200\n",
      "Response Keys: ['nhits', 'parameters', 'datasets']\n",
      "\n",
      "Sample Dataset Structure:\n",
      "  - datasetid: <class 'str'>\n",
      "  - metas: <class 'dict'>\n",
      "  - has_records: <class 'bool'>\n",
      "  - data_visible: <class 'bool'>\n",
      "  - features: <class 'list'>\n",
      "  - attachments: <class 'list'>\n",
      "  - alternative_exports: <class 'list'>\n",
      "  - fields: <class 'list'>\n",
      "  - basic_metas: <class 'dict'>\n",
      "  - interop_metas: <class 'dict'>\n",
      "  - extra_metas: <class 'dict'>\n",
      "\n",
      "Available Metadata Fields:\n",
      "  - metas.domain: <class 'str'>\n",
      "  - metas.staged: <class 'bool'>\n",
      "  - metas.visibility: <class 'str'>\n",
      "  - metas.metadata_processed: <class 'str'>\n",
      "  - metas.modified: <class 'str'>\n",
      "  - metas.license: <class 'str'>\n",
      "  - metas.description: <class 'str'>\n",
      "  - metas.publisher: <class 'str'>\n",
      "  - metas.theme: <class 'list'>\n",
      "  - metas.title: <class 'str'>\n",
      "\n",
      "Total datasets available: 10,795\n"
     ]
    }
   ],
   "source": [
    "# Test the API connection\n",
    "test_params = PARAMS.copy()\n",
    "test_params['rows'] = 1\n",
    "\n",
    "try:\n",
    "    print(\"Testing API connection...\")\n",
    "    response = requests.get(BASE_URL, params=test_params, headers=HEADERS, timeout=30)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    test_data = response.json()\n",
    "    \n",
    "    print(f\"API Connection Successful\")\n",
    "    print(f\"Status Code: {response.status_code}\")\n",
    "    print(f\"Response Keys: {list(test_data.keys())}\")\n",
    "    \n",
    "    if 'datasets' in test_data and len(test_data['datasets']) > 0:\n",
    "        print(\"\\nSample Dataset Structure:\")\n",
    "        sample_dataset = test_data['datasets'][0]\n",
    "        for key in sample_dataset.keys():\n",
    "            print(f\"  - {key}: {type(sample_dataset[key])}\")\n",
    "        \n",
    "        if 'metas' in sample_dataset:\n",
    "            print(\"\\nAvailable Metadata Fields:\")\n",
    "            metas = sample_dataset['metas']\n",
    "            for key in list(metas.keys())[:10]:\n",
    "                print(f\"  - metas.{key}: {type(metas[key])}\")\n",
    "    \n",
    "    total_datasets = test_data.get('nhits', 0)\n",
    "    print(f\"\\nTotal datasets available: {total_datasets:,}\")\n",
    "    \n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"API Connection Failed: {e}\")\n",
    "    print(\"Please check your internet connection and API endpoint.\")\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Extraction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pndb_data(max_records: int = None) -> List[Dict[Any, Any]]:\n",
    "    \"\"\"\n",
    "    Extract data from PNDB API with pagination support.\n",
    "    \n",
    "    Args:\n",
    "        max_records: Maximum number of records to extract (None for all)\n",
    "    \n",
    "    Returns:\n",
    "        List of dataset dictionaries\n",
    "    \"\"\"\n",
    "    all_datasets = []\n",
    "    start = 0\n",
    "    rows = 100\n",
    "    \n",
    "    print(f\"Starting data extraction from PNDB API...\")\n",
    "    print(f\"Target: {'All available records' if max_records is None else f'{max_records:,} records'}\")\n",
    "    \n",
    "    while True:\n",
    "        current_params = PARAMS.copy()\n",
    "        current_params['start'] = start\n",
    "        current_params['rows'] = rows\n",
    "        \n",
    "        try:\n",
    "            print(f\"\\nFetching records {start + 1:,} to {start + rows:,}...\")\n",
    "            response = requests.get(BASE_URL, params=current_params, headers=HEADERS, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            data = response.json()\n",
    "            current_results = data.get('datasets', [])\n",
    "            \n",
    "            if not current_results:\n",
    "                print(\"No more results found. Extraction complete.\")\n",
    "                break\n",
    "            \n",
    "            all_datasets.extend(current_results)\n",
    "            print(f\"Retrieved {len(current_results)} records. Total so far: {len(all_datasets):,}\")\n",
    "            \n",
    "            if max_records and len(all_datasets) >= max_records:\n",
    "                all_datasets = all_datasets[:max_records]\n",
    "                print(f\"Reached target of {max_records:,} records.\")\n",
    "                break\n",
    "            \n",
    "            total_count = data.get('nhits', 0)\n",
    "            if start + rows >= total_count:\n",
    "                print(f\"Reached end of available data ({total_count:,} total records).\")\n",
    "                break\n",
    "            \n",
    "            start += rows\n",
    "            time.sleep(0.1)\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error during API request: {e}\")\n",
    "            break\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error parsing JSON response: {e}\")\n",
    "            break\n",
    "    \n",
    "    print(f\"\\nExtraction completed! Total records extracted: {len(all_datasets):,}\")\n",
    "    return all_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset_metadata(datasets: List[Dict]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process raw dataset metadata into a structured DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        datasets: List of raw dataset dictionaries from API\n",
    "    \n",
    "    Returns:\n",
    "        Processed pandas DataFrame\n",
    "    \"\"\"\n",
    "    processed_data = []\n",
    "    \n",
    "    print(f\"Processing {len(datasets):,} datasets...\")\n",
    "    \n",
    "    for i, dataset in enumerate(datasets):\n",
    "        try:\n",
    "            metas = dataset.get('metas', {})\n",
    "            \n",
    "            title = metas.get('title', dataset.get('title', ''))\n",
    "            description = metas.get('description', dataset.get('description', ''))\n",
    "            creator = metas.get('publisher', metas.get('creator', metas.get('source', '')))\n",
    "            theme = metas.get('theme', metas.get('keyword', ''))\n",
    "            keywords = metas.get('keyword', metas.get('keywords', ''))\n",
    "            language = metas.get('language', 'fr')\n",
    "            geographic_coverage = metas.get('geographic_coverage', metas.get('spatial', ''))\n",
    "            \n",
    "            if isinstance(theme, list):\n",
    "                theme = '; '.join(theme)\n",
    "            if isinstance(keywords, list):\n",
    "                keywords = '; '.join(keywords)\n",
    "            \n",
    "            # Clean HTML from description\n",
    "            clean_description = re.sub(r'<[^>]+>', ' ', str(description))\n",
    "            clean_description = re.sub(r'\\s+', ' ', clean_description).strip()\n",
    "            \n",
    "            dataset_info = {\n",
    "                'dataset_id': dataset.get('datasetid', ''),\n",
    "                'title': str(title).strip(),\n",
    "                'description': clean_description,\n",
    "                'creator': str(creator).strip(),\n",
    "                'theme': str(theme).strip(),\n",
    "                'keywords': str(keywords).strip(),\n",
    "                'language': str(language).strip(),\n",
    "                'geographic_coverage': str(geographic_coverage).strip(),\n",
    "                'records_count': metas.get('records_count', 0),\n",
    "                'modified': metas.get('modified', ''),\n",
    "                'publisher': metas.get('publisher', ''),\n",
    "                'license': metas.get('license', ''),\n",
    "                'territory': ', '.join(metas.get('territory', [])) if isinstance(metas.get('territory'), list) else str(metas.get('territory', ''))\n",
    "            }\n",
    "            \n",
    "            processed_data.append(dataset_info)\n",
    "            \n",
    "            if (i + 1) % 1000 == 0:\n",
    "                print(f\"  Processed {i + 1:,}/{len(datasets):,} datasets...\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing dataset {i}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    df = pd.DataFrame(processed_data)\n",
    "    \n",
    "    print(f\"\\nProcessing completed\")\n",
    "    print(f\"DataFrame shape: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Execute Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PNDB Data Extraction Started ===\n",
      "Timestamp: 2025-11-06 12:49:17.628455\n",
      "Starting data extraction from PNDB API...\n",
      "Target: All available records\n",
      "\n",
      "Fetching records 1 to 100...\n",
      "Retrieved 100 records. Total so far: 100\n",
      "\n",
      "Fetching records 101 to 200...\n",
      "Retrieved 100 records. Total so far: 200\n",
      "\n",
      "Fetching records 201 to 300...\n",
      "Retrieved 100 records. Total so far: 300\n",
      "\n",
      "Fetching records 301 to 400...\n",
      "Retrieved 100 records. Total so far: 400\n",
      "\n",
      "Fetching records 401 to 500...\n",
      "Retrieved 100 records. Total so far: 500\n",
      "\n",
      "Fetching records 501 to 600...\n",
      "Retrieved 100 records. Total so far: 600\n",
      "\n",
      "Fetching records 601 to 700...\n",
      "Retrieved 100 records. Total so far: 700\n",
      "\n",
      "Fetching records 701 to 800...\n",
      "Retrieved 100 records. Total so far: 800\n",
      "\n",
      "Fetching records 801 to 900...\n",
      "Retrieved 100 records. Total so far: 900\n",
      "\n",
      "Fetching records 901 to 1,000...\n",
      "Retrieved 100 records. Total so far: 1,000\n",
      "\n",
      "Fetching records 1,001 to 1,100...\n",
      "Retrieved 100 records. Total so far: 1,100\n",
      "\n",
      "Fetching records 1,101 to 1,200...\n",
      "Retrieved 100 records. Total so far: 1,200\n",
      "\n",
      "Fetching records 1,201 to 1,300...\n",
      "Retrieved 100 records. Total so far: 1,300\n",
      "\n",
      "Fetching records 1,301 to 1,400...\n",
      "Retrieved 100 records. Total so far: 1,400\n",
      "\n",
      "Fetching records 1,401 to 1,500...\n",
      "Retrieved 100 records. Total so far: 1,500\n",
      "\n",
      "Fetching records 1,501 to 1,600...\n",
      "Retrieved 100 records. Total so far: 1,600\n",
      "\n",
      "Fetching records 1,601 to 1,700...\n",
      "Retrieved 100 records. Total so far: 1,700\n",
      "\n",
      "Fetching records 1,701 to 1,800...\n",
      "Retrieved 100 records. Total so far: 1,800\n",
      "\n",
      "Fetching records 1,801 to 1,900...\n",
      "Retrieved 100 records. Total so far: 1,900\n",
      "\n",
      "Fetching records 1,901 to 2,000...\n",
      "Retrieved 100 records. Total so far: 2,000\n",
      "\n",
      "Fetching records 2,001 to 2,100...\n",
      "Retrieved 100 records. Total so far: 2,100\n",
      "\n",
      "Fetching records 2,101 to 2,200...\n",
      "Retrieved 100 records. Total so far: 2,200\n",
      "\n",
      "Fetching records 2,201 to 2,300...\n",
      "Retrieved 100 records. Total so far: 2,300\n",
      "\n",
      "Fetching records 2,301 to 2,400...\n",
      "Retrieved 100 records. Total so far: 2,400\n",
      "\n",
      "Fetching records 2,401 to 2,500...\n",
      "Retrieved 100 records. Total so far: 2,500\n",
      "\n",
      "Fetching records 2,501 to 2,600...\n",
      "Retrieved 100 records. Total so far: 2,600\n",
      "\n",
      "Fetching records 2,601 to 2,700...\n",
      "Retrieved 100 records. Total so far: 2,700\n",
      "\n",
      "Fetching records 2,701 to 2,800...\n",
      "Retrieved 100 records. Total so far: 2,800\n",
      "\n",
      "Fetching records 2,801 to 2,900...\n",
      "Retrieved 100 records. Total so far: 2,900\n",
      "\n",
      "Fetching records 2,901 to 3,000...\n",
      "Retrieved 100 records. Total so far: 3,000\n",
      "\n",
      "Fetching records 3,001 to 3,100...\n",
      "Retrieved 100 records. Total so far: 3,100\n",
      "\n",
      "Fetching records 3,101 to 3,200...\n",
      "Retrieved 100 records. Total so far: 3,200\n",
      "\n",
      "Fetching records 3,201 to 3,300...\n",
      "Retrieved 100 records. Total so far: 3,300\n",
      "\n",
      "Fetching records 3,301 to 3,400...\n",
      "Retrieved 100 records. Total so far: 3,400\n",
      "\n",
      "Fetching records 3,401 to 3,500...\n",
      "Retrieved 100 records. Total so far: 3,500\n",
      "\n",
      "Fetching records 3,501 to 3,600...\n",
      "Retrieved 100 records. Total so far: 3,600\n",
      "\n",
      "Fetching records 3,601 to 3,700...\n",
      "Retrieved 100 records. Total so far: 3,700\n",
      "\n",
      "Fetching records 3,701 to 3,800...\n",
      "Retrieved 100 records. Total so far: 3,800\n",
      "\n",
      "Fetching records 3,801 to 3,900...\n",
      "Retrieved 100 records. Total so far: 3,900\n",
      "\n",
      "Fetching records 3,901 to 4,000...\n",
      "Retrieved 100 records. Total so far: 4,000\n",
      "\n",
      "Fetching records 4,001 to 4,100...\n",
      "Retrieved 100 records. Total so far: 4,100\n",
      "\n",
      "Fetching records 4,101 to 4,200...\n",
      "Retrieved 100 records. Total so far: 4,200\n",
      "\n",
      "Fetching records 4,201 to 4,300...\n",
      "Retrieved 100 records. Total so far: 4,300\n",
      "\n",
      "Fetching records 4,301 to 4,400...\n",
      "Retrieved 100 records. Total so far: 4,400\n",
      "\n",
      "Fetching records 4,401 to 4,500...\n",
      "Retrieved 100 records. Total so far: 4,500\n",
      "\n",
      "Fetching records 4,501 to 4,600...\n",
      "Retrieved 100 records. Total so far: 4,600\n",
      "\n",
      "Fetching records 4,601 to 4,700...\n",
      "Retrieved 100 records. Total so far: 4,700\n",
      "\n",
      "Fetching records 4,701 to 4,800...\n",
      "Retrieved 100 records. Total so far: 4,800\n",
      "\n",
      "Fetching records 4,801 to 4,900...\n",
      "Retrieved 100 records. Total so far: 4,900\n",
      "\n",
      "Fetching records 4,901 to 5,000...\n",
      "Retrieved 100 records. Total so far: 5,000\n",
      "\n",
      "Fetching records 5,001 to 5,100...\n",
      "Retrieved 100 records. Total so far: 5,100\n",
      "\n",
      "Fetching records 5,101 to 5,200...\n",
      "Retrieved 100 records. Total so far: 5,200\n",
      "\n",
      "Fetching records 5,201 to 5,300...\n",
      "Retrieved 100 records. Total so far: 5,300\n",
      "\n",
      "Fetching records 5,301 to 5,400...\n",
      "Retrieved 100 records. Total so far: 5,400\n",
      "\n",
      "Fetching records 5,401 to 5,500...\n",
      "Retrieved 100 records. Total so far: 5,500\n",
      "\n",
      "Fetching records 5,501 to 5,600...\n",
      "Retrieved 100 records. Total so far: 5,600\n",
      "\n",
      "Fetching records 5,601 to 5,700...\n",
      "Retrieved 100 records. Total so far: 5,700\n",
      "\n",
      "Fetching records 5,701 to 5,800...\n",
      "Retrieved 100 records. Total so far: 5,800\n",
      "\n",
      "Fetching records 5,801 to 5,900...\n",
      "Retrieved 100 records. Total so far: 5,900\n",
      "\n",
      "Fetching records 5,901 to 6,000...\n",
      "Retrieved 100 records. Total so far: 6,000\n",
      "\n",
      "Fetching records 6,001 to 6,100...\n",
      "Retrieved 100 records. Total so far: 6,100\n",
      "\n",
      "Fetching records 6,101 to 6,200...\n",
      "Retrieved 100 records. Total so far: 6,200\n",
      "\n",
      "Fetching records 6,201 to 6,300...\n",
      "Retrieved 100 records. Total so far: 6,300\n",
      "\n",
      "Fetching records 6,301 to 6,400...\n",
      "Retrieved 100 records. Total so far: 6,400\n",
      "\n",
      "Fetching records 6,401 to 6,500...\n",
      "Retrieved 100 records. Total so far: 6,500\n",
      "\n",
      "Fetching records 6,501 to 6,600...\n",
      "Retrieved 100 records. Total so far: 6,600\n",
      "\n",
      "Fetching records 6,601 to 6,700...\n",
      "Retrieved 100 records. Total so far: 6,700\n",
      "\n",
      "Fetching records 6,701 to 6,800...\n",
      "Retrieved 100 records. Total so far: 6,800\n",
      "\n",
      "Fetching records 6,801 to 6,900...\n",
      "Retrieved 100 records. Total so far: 6,900\n",
      "\n",
      "Fetching records 6,901 to 7,000...\n",
      "Retrieved 100 records. Total so far: 7,000\n",
      "\n",
      "Fetching records 7,001 to 7,100...\n",
      "Retrieved 100 records. Total so far: 7,100\n",
      "\n",
      "Fetching records 7,101 to 7,200...\n",
      "Retrieved 100 records. Total so far: 7,200\n",
      "\n",
      "Fetching records 7,201 to 7,300...\n",
      "Retrieved 100 records. Total so far: 7,300\n",
      "\n",
      "Fetching records 7,301 to 7,400...\n",
      "Retrieved 100 records. Total so far: 7,400\n",
      "\n",
      "Fetching records 7,401 to 7,500...\n",
      "Retrieved 100 records. Total so far: 7,500\n",
      "\n",
      "Fetching records 7,501 to 7,600...\n",
      "Retrieved 100 records. Total so far: 7,600\n",
      "\n",
      "Fetching records 7,601 to 7,700...\n",
      "Retrieved 100 records. Total so far: 7,700\n",
      "\n",
      "Fetching records 7,701 to 7,800...\n",
      "Retrieved 100 records. Total so far: 7,800\n",
      "\n",
      "Fetching records 7,801 to 7,900...\n",
      "Retrieved 100 records. Total so far: 7,900\n",
      "\n",
      "Fetching records 7,901 to 8,000...\n",
      "Retrieved 100 records. Total so far: 8,000\n",
      "\n",
      "Fetching records 8,001 to 8,100...\n",
      "Retrieved 100 records. Total so far: 8,100\n",
      "\n",
      "Fetching records 8,101 to 8,200...\n",
      "Retrieved 100 records. Total so far: 8,200\n",
      "\n",
      "Fetching records 8,201 to 8,300...\n",
      "Retrieved 100 records. Total so far: 8,300\n",
      "\n",
      "Fetching records 8,301 to 8,400...\n",
      "Retrieved 100 records. Total so far: 8,400\n",
      "\n",
      "Fetching records 8,401 to 8,500...\n",
      "Retrieved 100 records. Total so far: 8,500\n",
      "\n",
      "Fetching records 8,501 to 8,600...\n",
      "Retrieved 100 records. Total so far: 8,600\n",
      "\n",
      "Fetching records 8,601 to 8,700...\n",
      "Retrieved 100 records. Total so far: 8,700\n",
      "\n",
      "Fetching records 8,701 to 8,800...\n",
      "Retrieved 100 records. Total so far: 8,800\n",
      "\n",
      "Fetching records 8,801 to 8,900...\n",
      "Retrieved 100 records. Total so far: 8,900\n",
      "\n",
      "Fetching records 8,901 to 9,000...\n",
      "Retrieved 100 records. Total so far: 9,000\n",
      "\n",
      "Fetching records 9,001 to 9,100...\n",
      "Retrieved 100 records. Total so far: 9,100\n",
      "\n",
      "Fetching records 9,101 to 9,200...\n",
      "Retrieved 100 records. Total so far: 9,200\n",
      "\n",
      "Fetching records 9,201 to 9,300...\n",
      "Retrieved 100 records. Total so far: 9,300\n",
      "\n",
      "Fetching records 9,301 to 9,400...\n",
      "Retrieved 100 records. Total so far: 9,400\n",
      "\n",
      "Fetching records 9,401 to 9,500...\n",
      "Retrieved 100 records. Total so far: 9,500\n",
      "\n",
      "Fetching records 9,501 to 9,600...\n",
      "Retrieved 100 records. Total so far: 9,600\n",
      "\n",
      "Fetching records 9,601 to 9,700...\n",
      "Retrieved 100 records. Total so far: 9,700\n",
      "\n",
      "Fetching records 9,701 to 9,800...\n",
      "Retrieved 100 records. Total so far: 9,800\n",
      "\n",
      "Fetching records 9,801 to 9,900...\n",
      "Retrieved 100 records. Total so far: 9,900\n",
      "\n",
      "Fetching records 9,901 to 10,000...\n",
      "Retrieved 100 records. Total so far: 10,000\n",
      "\n",
      "Fetching records 10,001 to 10,100...\n",
      "No more results found. Extraction complete.\n",
      "\n",
      "Extraction completed! Total records extracted: 10,000\n",
      "\n",
      "Raw extraction completed: 10,000 datasets retrieved\n"
     ]
    }
   ],
   "source": [
    "print(\"=== PNDB Data Extraction Started ===\")\n",
    "print(f\"Timestamp: {datetime.now()}\")\n",
    "\n",
    "# Extract all available datasets\n",
    "raw_datasets = extract_pndb_data(max_records=None)\n",
    "\n",
    "print(f\"\\nRaw extraction completed: {len(raw_datasets):,} datasets retrieved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Process and Structure the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 10,000 datasets...\n",
      "  Processed 1,000/10,000 datasets...\n",
      "  Processed 2,000/10,000 datasets...\n",
      "  Processed 3,000/10,000 datasets...\n",
      "  Processed 4,000/10,000 datasets...\n",
      "  Processed 5,000/10,000 datasets...\n",
      "  Processed 6,000/10,000 datasets...\n",
      "  Processed 7,000/10,000 datasets...\n",
      "  Processed 8,000/10,000 datasets...\n",
      "  Processed 9,000/10,000 datasets...\n",
      "  Processed 10,000/10,000 datasets...\n",
      "\n",
      "Processing completed\n",
      "DataFrame shape: (10000, 13)\n",
      "Columns: ['dataset_id', 'title', 'description', 'creator', 'theme', 'keywords', 'language', 'geographic_coverage', 'records_count', 'modified', 'publisher', 'license', 'territory']\n",
      "\n",
      "=== Data Processing Results ===\n",
      "Total datasets processed: 10,000\n",
      "DataFrame shape: (10000, 13)\n",
      "\n",
      "First 3 rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>creator</th>\n",
       "      <th>theme</th>\n",
       "      <th>keywords</th>\n",
       "      <th>language</th>\n",
       "      <th>geographic_coverage</th>\n",
       "      <th>records_count</th>\n",
       "      <th>modified</th>\n",
       "      <th>publisher</th>\n",
       "      <th>license</th>\n",
       "      <th>territory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dendrophyllia-cornigera-scleractinia-presence-...</td>\n",
       "      <td>Dendrophyllia cornigera (Scleractinia) presenc...</td>\n",
       "      <td>Lien vers les données Data were extracted from...</td>\n",
       "      <td>Ifremer</td>\n",
       "      <td>Species distribution</td>\n",
       "      <td></td>\n",
       "      <td>fr</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2021-06-14T14:27:24+00:00</td>\n",
       "      <td>Ifremer</td>\n",
       "      <td>CC BY-NC-SA 4.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>suivi-floristiques-en-exclos-marais-de-sougeal...</td>\n",
       "      <td>Suivi floristiques en exclos (Marais de Sougea...</td>\n",
       "      <td>Lien vers les données Objectif : Cette tâche v...</td>\n",
       "      <td>ECOBIO UMR 6553 CNRS Université de Rennes 1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>fr</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2018-06-15T09:18:52+00:00</td>\n",
       "      <td>ECOBIO UMR 6553 CNRS Université de Rennes 1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>suivi-floristique-observation-aquatique-marais...</td>\n",
       "      <td>Suivi floristique -observation aquatique- (Mar...</td>\n",
       "      <td>Lien vers les données Cette opération vise à m...</td>\n",
       "      <td>ECOBIO UMR 6553 CNRS Université de Rennes 1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>fr</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2018-03-15T14:25:51+00:00</td>\n",
       "      <td>ECOBIO UMR 6553 CNRS Université de Rennes 1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          dataset_id  \\\n",
       "0  dendrophyllia-cornigera-scleractinia-presence-...   \n",
       "1  suivi-floristiques-en-exclos-marais-de-sougeal...   \n",
       "2  suivi-floristique-observation-aquatique-marais...   \n",
       "\n",
       "                                               title  \\\n",
       "0  Dendrophyllia cornigera (Scleractinia) presenc...   \n",
       "1  Suivi floristiques en exclos (Marais de Sougea...   \n",
       "2  Suivi floristique -observation aquatique- (Mar...   \n",
       "\n",
       "                                         description  \\\n",
       "0  Lien vers les données Data were extracted from...   \n",
       "1  Lien vers les données Objectif : Cette tâche v...   \n",
       "2  Lien vers les données Cette opération vise à m...   \n",
       "\n",
       "                                       creator                 theme keywords  \\\n",
       "0                                      Ifremer  Species distribution            \n",
       "1  ECOBIO UMR 6553 CNRS Université de Rennes 1                                  \n",
       "2  ECOBIO UMR 6553 CNRS Université de Rennes 1                                  \n",
       "\n",
       "  language geographic_coverage  records_count                   modified  \\\n",
       "0       fr                                  0  2021-06-14T14:27:24+00:00   \n",
       "1       fr                                  0  2018-06-15T09:18:52+00:00   \n",
       "2       fr                                  0  2018-03-15T14:25:51+00:00   \n",
       "\n",
       "                                     publisher          license territory  \n",
       "0                                      Ifremer  CC BY-NC-SA 4.0            \n",
       "1  ECOBIO UMR 6553 CNRS Université de Rennes 1                             \n",
       "2  ECOBIO UMR 6553 CNRS Université de Rennes 1                             "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data types:\n",
      "dataset_id             object\n",
      "title                  object\n",
      "description            object\n",
      "creator                object\n",
      "theme                  object\n",
      "keywords               object\n",
      "language               object\n",
      "geographic_coverage    object\n",
      "records_count           int64\n",
      "modified               object\n",
      "publisher              object\n",
      "license                object\n",
      "territory              object\n",
      "dtype: object\n",
      "\n",
      "Basic statistics:\n",
      "  Non-empty titles: 10,000\n",
      "  Non-empty descriptions: 10,000\n",
      "  Average records per dataset: 0.01\n"
     ]
    }
   ],
   "source": [
    "if raw_datasets:\n",
    "    df_pndb = process_dataset_metadata(raw_datasets)\n",
    "    \n",
    "    print(\"\\n=== Data Processing Results ===\")\n",
    "    print(f\"Total datasets processed: {len(df_pndb):,}\")\n",
    "    print(f\"DataFrame shape: {df_pndb.shape}\")\n",
    "    \n",
    "    print(\"\\nFirst 3 rows:\")\n",
    "    display(df_pndb.head(3))\n",
    "    \n",
    "    print(\"\\nData types:\")\n",
    "    print(df_pndb.dtypes)\n",
    "    \n",
    "    print(\"\\nBasic statistics:\")\n",
    "    print(f\"  Non-empty titles: {df_pndb['title'].notna().sum():,}\")\n",
    "    print(f\"  Non-empty descriptions: {df_pndb['description'].notna().sum():,}\")\n",
    "    print(f\"  Average records per dataset: {df_pndb['records_count'].mean():.2f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No data was extracted. Please check the API connection and parameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Data to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Export Completed \n",
      "File saved as: pndb_metadata_extracted_20251106_125020.csv\n",
      "File size: 7.66 MB\n",
      "Records saved: 10,000\n",
      "Columns saved: 13\n",
      "File successfully created and verified\n",
      "File can be read back successfully\n",
      "Sample verification: 5 rows read\n"
     ]
    }
   ],
   "source": [
    "if 'df_pndb' in locals() and not df_pndb.empty:\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    csv_filename = f\"pndb_metadata_extracted_{timestamp}.csv\"\n",
    "    \n",
    "    try:\n",
    "        df_pndb.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "        \n",
    "        print(f\"Data Export Completed \")\n",
    "        print(f\"File saved as: {csv_filename}\")\n",
    "        print(f\"File size: {os.path.getsize(csv_filename) / 1024 / 1024:.2f} MB\")\n",
    "        print(f\"Records saved: {len(df_pndb):,}\")\n",
    "        print(f\"Columns saved: {len(df_pndb.columns)}\")\n",
    "        \n",
    "        if os.path.exists(csv_filename):\n",
    "            print(f\"File successfully created and verified\")\n",
    "            \n",
    "            test_df = pd.read_csv(csv_filename, encoding='utf-8-sig', nrows=5)\n",
    "            print(f\"File can be read back successfully\")\n",
    "            print(f\"Sample verification: {len(test_df)} rows read\")\n",
    "        else:\n",
    "            print(\"Error: File was not created\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving to CSV: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"No data available to save.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PNDB Data Extraction Summary\n",
      "Extraction completed at: 2025-11-06 12:52:55.334577\n",
      "\n",
      "Results:\n",
      "  Total datasets extracted: 10,000\n",
      "  Data fields captured: 13\n",
      "  CSV file created: pndb_metadata_extracted_20251106_125020.csv\n",
      "\n",
      "Next Steps:\n",
      "  1. Apply Named Entity Recognition (NER) to extract species names\n",
      "  2. Filter datasets by geographic regions of interest\n",
      "  3. Create visualizations of data distribution and themes\n",
      "  4. Explore individual datasets for detailed biodiversity data\n",
      "\n",
      "Technical Notes:\n",
      "  API Endpoint: https://pndb.opendatasoft.com/api/datasets/1.0/search/\n",
      "  Pagination: 100 records per request\n",
      "  Encoding: UTF-8 with BOM for French text\n",
      "  Rate limiting: 0.1s delay between requests\n",
      "  API Format: OpenDataSoft v1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"PNDB Data Extraction Summary\")\n",
    "print(f\"Extraction completed at: {datetime.now()}\")\n",
    "\n",
    "if 'df_pndb' in locals() and not df_pndb.empty:\n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"  Total datasets extracted: {len(df_pndb):,}\")\n",
    "    print(f\"  Data fields captured: {len(df_pndb.columns)}\")\n",
    "    print(f\"  CSV file created: {csv_filename if 'csv_filename' in locals() else 'Not created'}\")\n",
    "    \n",
    "    print(f\"\\nNext Steps:\")\n",
    "    print(f\"  1. Apply Named Entity Recognition (NER) to extract species names\")\n",
    "    print(f\"  2. Filter datasets by geographic regions of interest\")\n",
    "    print(f\"  3. Create visualizations of data distribution and themes\")\n",
    "    print(f\"  4. Explore individual datasets for detailed biodiversity data\")\n",
    "    \n",
    "    print(f\"\\nTechnical Notes:\")\n",
    "    print(f\"  API Endpoint: {BASE_URL}\")\n",
    "    print(f\"  Pagination: 100 records per request\")\n",
    "    print(f\"  Encoding: UTF-8 with BOM for French text\")\n",
    "    print(f\"  Rate limiting: 0.1s delay between requests\")\n",
    "    print(f\"  API Format: OpenDataSoft v1.0\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\nNo data was successfully extracted.\")\n",
    "    print(\"Please check:\")\n",
    "    print(\"  Internet connection\")\n",
    "    print(\"  API endpoint availability\")\n",
    "    print(\"  API key validity\")\n",
    "    print(\"  Request parameters\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
